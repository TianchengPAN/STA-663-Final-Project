{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data, batch_size):\n",
    "    '''Define a function to compute the minibatches'''\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    \"Stochastic Gradient Hamiltonian Monte Carlo\"\n",
    "    by Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    ICML (2014).\n",
    "    \n",
    "    The inputs include:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    # initialize parameters and check if the inputs work\n",
    "    p = len(theta_0) # get dimension of theta_0\n",
    "    n = dat.shape[0] # get the row number of dat\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size))) # set up matrix of 0s to hold samples\n",
    "    # compute beta_hat and Sigma as described on page 6 in the SGHMC paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    # check if the Sigma is a positive definite matrix\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta is not positive definite\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1) # initialize nu\n",
    "    theta = theta_0 # initialize theta\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            grad_U_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ grad_U_batch - alpha @ nu + \\\n",
    "                 np.random.multivariate_normal(np.zeros(p), Sigma).reshape(p, -1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_cleaned(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "    The inputs are:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta is not positive definite\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "\n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "\n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.sqrt(eta_chol) @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            grad_U_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ grad_U_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from multivariate normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization, try to use JIT first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def minibatch_numba(data, batch_size):\n",
    "    '''Define a function to compute the minibatches'''\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    \"Stochastic Gradient Hamiltonian Monte Carlo\"\n",
    "    by Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    ICML (2014).\n",
    "    \n",
    "    The inputs include:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    # initialize parameters and check if the inputs work\n",
    "    p = len(theta_0) # get dimension of theta_0\n",
    "    n = dat.shape[0] # get the row number of dat\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size))) # set up matrix of 0s to hold samples\n",
    "    # compute beta_hat and Sigma as described on page 6 in the SGHMC paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    # check if the Sigma is a positive definite matrix\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta is not positive definite\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1) # initialize nu\n",
    "    theta = theta_0 # initialize theta\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            grad_U_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ grad_U_batch - alpha @ nu + \\\n",
    "                 np.random.multivariate_normal(np.zeros(p), Sigma).reshape(p, -1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_cleaned(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "    The inputs are:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta is not positive definite\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "\n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "\n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.sqrt(eta_chol) @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            grad_U_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ grad_U_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from multivariate normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparsion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting project.cpp\n"
     ]
    }
   ],
   "source": [
    "%%file project.cpp\n",
    "<%\n",
    "cfg['compiler_args'] = ['-std=c++11']\n",
    "cfg['include_dirs'] = ['../../eigen3']\n",
    "setup_pybind11(cfg)\n",
    "%>\n",
    "\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/eigen.h>\n",
    "#include <stdexcept>\n",
    "#include <algorithm> // std::random_shuffle\n",
    "#include <random>\n",
    "\n",
    "#include <Eigen/LU>\n",
    "#include <Eigen/Dense>\n",
    "\n",
    "namespace py = pybind11;\n",
    "using std::default_random_engine;\n",
    "using std::normal_distribution;\n",
    "        \n",
    "default_random_engine re{100};\n",
    "normal_distribution<double> norm(0, 1);\n",
    "auto rnorm = bind(norm, re);\n",
    "\n",
    "Eigen::MatrixXd rnorm_vec(int n) {\n",
    "    Eigen::MatrixXd res_vec = Eigen::MatrixXd::Zero(n, 1);\n",
    "    for (int i=0; i<n; i++) {\n",
    "        res_vec(i,0) = rnorm();\n",
    "    }\n",
    "    return res_vec;\n",
    "}\n",
    "     \n",
    "Eigen::MatrixXd gradU_mixNormals(Eigen::MatrixXd theta, Eigen::MatrixXd x, int n, int batch_size) {\n",
    "    // calculate gradient for mixture of normals example\n",
    "    int p = theta.rows();\n",
    "    Eigen::ArrayXd c_0 = theta(0,0) - x.array();\n",
    "    Eigen::ArrayXd c_1 = theta(1,0) - x.array();\n",
    "    Eigen::ArrayXd star = 0.5 * (-0.5 * c_0.pow(2)).exp() + 0.5 * (-0.5 * c_1.pow(2)).exp();\n",
    "    Eigen::ArrayXd star_prime;\n",
    "    Eigen::MatrixXd grad = Eigen::MatrixXd::Zero(p, 1);\n",
    "    for (int i=0; i<p; i++) {\n",
    "        star_prime = 0.5 * (-0.5 * (theta(i,0) - x.array()).pow(2)).exp() * (theta(i,0) - x.array());\n",
    "        grad(i,0) = -theta(i,0)/10 - (n/batch_size)*(star_prime/star).sum();\n",
    "    }\n",
    "    return grad;\n",
    "} \n",
    "\n",
    "Eigen::MatrixXd sghmc(std::string gradU_choice, Eigen::MatrixXd eta, int niter, Eigen::MatrixXd alpha, Eigen::MatrixXd theta_0, Eigen::MatrixXd V_hat, Eigen::MatrixXd dat, int batch_size){\n",
    "    int p = theta_0.rows(); // dimension of the thing you're sampling\n",
    "    int n = dat.rows();     // number of data observations\n",
    "    int p_dat = dat.cols(); // 2nd dimension of data\n",
    "    int nbatches = n / batch_size; // how many batches data will be broken into\n",
    "    Eigen::MatrixXd dat_temp = dat;\n",
    "    Eigen::MatrixXd dat_batch = Eigen::MatrixXd::Zero(batch_size, p_dat);\n",
    "    Eigen::MatrixXd gradU_batch = Eigen::MatrixXd::Zero(p, 1);\n",
    "    Eigen::MatrixXd theta_samps = Eigen::MatrixXd::Zero(p, niter*(n/batch_size));\n",
    "    std::vector<int> ind;\n",
    "    // fix beta_hat as described on pg. 6 of paper\n",
    "    Eigen::MatrixXd beta_hat = 0.5 * V_hat * eta;\n",
    "    // We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    // so this must be a positive definite matrix\n",
    "    Eigen::MatrixXd Sigma = 2.0 * (alpha - beta_hat) * eta;\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfSig(Sigma); // compute the Cholesky decomposition of Sigma\n",
    "    if(lltOfSig.info() == Eigen::NumericalIssue){ // check if we got error, and break out if so\n",
    "        return theta_samps; // will just give back all-0s\n",
    "    }\n",
    "    Eigen::MatrixXd Sig_chol = lltOfSig.matrixL(); // get L in Chol decomp\n",
    "    if(batch_size > n){ // Need batch size to be <= the amount of data\n",
    "        return theta_samps; // will just give back all-0s\n",
    "    }\n",
    "    // initialize more things! (nu and theta, to be specific)\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfeta(eta); // compute the Cholesky decomposition of eta\n",
    "    Eigen::MatrixXd eta_chol = lltOfeta.matrixL(); // get L in Chol decomp\n",
    "    Eigen::MatrixXd nu = eta_chol * rnorm_vec(p); // initialize nu\n",
    "    Eigen::MatrixXd theta = theta_0; // initialize theta \n",
    "    \n",
    "    // loop through algorithm to get niter*batch_size samples\n",
    "    int big_iter = 0;\n",
    "    for (int it=0; it<niter; it++) {\n",
    "        \n",
    "        // shuffle rows of dat to get dat_temp \n",
    "        Eigen::VectorXi indices = Eigen::VectorXi::LinSpaced(dat.rows(), 0, dat.rows());\n",
    "        std::random_shuffle(indices.data(), indices.data() + dat.rows());\n",
    "        dat_temp = indices.asPermutation() * dat;\n",
    "        \n",
    "        // Resample momentum every epoch\n",
    "        nu = eta_chol * rnorm_vec(p); // sample from MV normal\n",
    "        \n",
    "        // loop through the batches\n",
    "        int count_lower = 0;\n",
    "        int count_upper = batch_size;\n",
    "        for (int b=0; b<nbatches; b++){\n",
    "            int batch_ind = 0;\n",
    "            for (int ind_temp=count_lower; ind_temp<count_upper; ind_temp++){\n",
    "                dat_batch.row(batch_ind) = dat_temp.row(ind_temp);\n",
    "                batch_ind += 1;\n",
    "            }\n",
    "            count_lower += batch_size; // add batch size to each iterator\n",
    "            count_upper += batch_size;\n",
    "            if (gradU_choice == \"fig1\"){\n",
    "                gradU_batch = gradU_noisyFig1(theta);\n",
    "            } else if (gradU_choice == \"mixture_of_normals\"){\n",
    "                gradU_batch = gradU_mixNormals(theta, dat_batch, n, batch_size);\n",
    "            } else {\n",
    "                return theta_samps; // will just give back all-0s\n",
    "            }\n",
    "            nu = nu - eta * gradU_batch - alpha * nu + Sig_chol * rnorm_vec(p);\n",
    "            theta = theta + nu;\n",
    "            theta_samps.col(big_iter) = theta;\n",
    "            big_iter += 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return theta_samps;\n",
    "}\n",
    "    \n",
    "PYBIND11_MODULE(project, m) {\n",
    "    m.doc() = \"auto-compiled c++ extension\";\n",
    "    m.def(\"sghmc\", &sghmc);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparsion\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
