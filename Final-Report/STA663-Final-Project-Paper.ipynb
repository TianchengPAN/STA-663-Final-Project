{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations of the Stochastic Gradient Hamilton Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Tiancheng Pan, Lingyu Zhou and Fan Zhu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this package, we followed and implemented the \"Stochastic Gradient Hamiltonian Monte Carlo\" algorithm, which was proposed by Tianqi Chen, Emily B. Fox and Carlos Guestrin (2014). To significantly reduce the computational complexity, this Hamilton Monte Carlo algorithm incorporates the stochastic gradient, which is computed on minibatchs of data with noise and counterbalances that noise by a friction term. The SGHMC algorithm was initially implemented in Python and then optimized through numba, Cython (C++) and multiprocessing. To examine whether our SGHMC algorithm works, we tested our codes on one simulated example similar to that in the original paper and one simulated example generated from a mixture normal distribution, for which we sample the posterior of the mean parameters. The behaviors of SGHMC algorithm were also compared with two competing Monte Carlo algorithms on our first simulated data. There is an up-to-date Github repository for our SGHMC codes at (https://github.com/TianchenPan/STA-663-Final-Project). The instructions of installation and explanation of this package are available in README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamiltonian Monte Carlo (HMC) algorithm was first propsed by Duane, S., Kennedy, A. D., Pendleton, B. J. and Roweth, D. in 1987. The Hamiltonian Monte Carlo sampling method basically treats the probability density as a physical system in which there exists a moving object. As the law of conservation of energy, there is a trade-off between potential energy and the momentum of the moving object. This process involves the computation of the gradient information into the proposal distribution, which, when the sample size is large, is computational costly. \n",
    "\n",
    "To tackle with this computational issue, Chen, T., Fox, E., & Guestrin, C. proposed the Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) algorithm in 2014. The SGHMC algorithm avoids the manifold computational efforts by applying a stochastic gradient on the minibatches of data with a certain degree noise and then counteracts that noise with a “friction” term, which can maintain the desired target distribution and invariance properties.\n",
    "\n",
    "The applications of the SGHMC are consistent with those of the Markov Chain Monte Carlo (MCMC) sampling methods. However, the SGHMC algorithm is preferred when the usual random walk methods tend to generate samples with highly correlated variables or samples with very low acceptance probabilities (Chen, T. et al, 2014). By using the minibatches of data, the SGHMC method also allows for scaling of Bayesian methods. Despite these advantages, Chen and his colleagues argued that there are two limitations of the SGHMC algorithm (2014). The first limitation is that the use of the minibatches may lead to inappropriate stochastic gradient because the subsample size may not be big enough. The other limitation is that, compared to other MCMC algorithms, the SGHMC method requires the approximations of more variables at first. Those different initializations may result in different target distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Stochastic Gradient Hamiltonian Monte Carlo by Chen, T., Fox, E., and Guestrin, C. in 2014, we can briefly described this algorithm as follows: \n",
    "\n",
    "First, with the help of auxiliary variables in physics system, the basic Hamiltonian function is defined by:\n",
    "\n",
    "$$ H(\\theta,r)=U(\\theta)+\\frac{1}{2}r^{T}M^{-1}r $$\n",
    "\n",
    "Where $U(\\theta)$ is the potential energy function defined by $U(\\theta)=-\\sum_{x \\in D}logp(x|\\theta)-logp(\\theta)$ given a set of independent observations $x \\in D$ while r and mass matrix M together define the kinetic energy term. Then, the unit change in $\\theta$ and $r$ is:\n",
    "\n",
    "$$ \\begin{align} d\\theta &= M^{-1}rdt \\ dr \\\\ \n",
    "&= -\\nabla U(\\theta)dt \\end{align} $$\n",
    "\n",
    "Chen et al introduced a stochastic gradient based on minibatch of data $\\tilde D$ with noise into system and counterbalanced that noise by a friction term (2014). Then the stochastic gradient is defined by:\n",
    "\n",
    "$$ \\nabla \\tilde{U}(\\theta)=-\\frac{|D|}{|\\tilde{D}|}\\sum_{x \\in \\tilde{D}}\\nabla logp(x|\\theta)-\\nabla logp(\\theta) $$\n",
    "\n",
    "The stochastic gradient with noise is based on the assumptions that $\\nabla \\tilde{U}(\\theta)$ follows a normal distribution \n",
    "$\\nabla \\tilde{U}(\\theta) = \\nabla U(\\theta) + N(0,V(\\theta))$, where $V(\\theta)$ is the covariance of stochastic gradient noise. Then SGHMC introduces an altered HMC procedure to counteract the noise term, in which the unit change in $\\theta$ and $r$ is defined by:\n",
    "\n",
    "$$ \\begin{align} d\\theta&=M^{-1}rdt \\ dr \\\\\n",
    "&=-\\nabla U(\\theta)dt-BM^{-1}rdt+N(0,2Bdt) \\end{align} $$\n",
    "\n",
    "For simplicity, $B(\\theta)$ is abbreviated to $B$. $N(0,2Bdt)$ is the noise approximated by a normal distribution where $B(\\theta)=\\frac{1}{2}\\epsilon V(\\theta)$ is the diffusion matrix contributed by gradient noise. $\\epsilon$ is the step size, and it takes similar function with learning rate in gradient descent. Thus, it is a very small number. $-BM^{-1}rdt$ is the friction term. However, in practice we rarely know the exact noise model $B$. We use $\\hat{B}$ instead to approximate noise model. As a result, a new friction term $C\\succeq \\hat{B}$ is introduced. The new equation of $dr$ is:\n",
    "\n",
    "$$ dr=-\\nabla U(\\theta)dt-CM^{-1}rdt+N(0,2(C-\\hat{B})dt)+N(0,2Bdt) $$\n",
    "\n",
    "Finally,the algorithm is:\n",
    "\n",
    "initialize $(\\theta_0, r_0,\\epsilon, M,\\hat{B},B,C)$\n",
    "\n",
    "when t=1,2,3......, we update $\\theta$ and $r$ with:\n",
    "\n",
    "$$ \\begin{align} \\theta_i &=\\theta_{i-1}+\\epsilon_tM^{-1}r_{i-1}\\ r_i \\\\\n",
    "&=r_{i-1}-\\epsilon_t \\nabla \\tilde {U}(\\theta_i)-\\epsilon_t CM^{-1}r_{i-1}+N(0,2(C-\\hat{B})\\epsilon_t) \\end{align} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Optimization for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Simulated Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1]( Example1_a.png \"Figure 1\")\n",
    "<center>Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first simulation example, which is as mentioned as in the original paper, where $U(\\theta)=-2\\theta^2+\\theta^4$ and $\\nabla \\tilde{U}=\\nabla U + N(0,4)$. We set $\\hat{V}$ to be a 0 matrix with shape $(1,1)$, $\\epsilon$ equals to 0.1, and batch size equal to 1. The sample size is 1000 and the number of iteration is 2000. Finally, we have a very similar density plot to that in the original paper. Thus the algorithm we have is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simualtion 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2]( Example2_a.png \"Figure 2\")\n",
    "<center>Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second simulation example, we choose the mixture normal distribution, where $x \\sim 0.5*N(\\mu_1,1)+0.5*N(\\mu_2,1)$, where $\\mu_1=-3, \\mu_2=3$.We set $\\hat{V}$ to be an identity matrix with shape $(2,2)$, $\\epsilon$ equals 0.1, and batch size equal to 1000. The sample size is 1000 and the number of iteration is 2000 again. The result is shown in figure 2, which is quite reasonable since the distribution is centered at $(3,-3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Real Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis with Competing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the comparative analysis, we want to focus on the same problem as simulation 1, since we have the exact density plot in the original paper. I want to compare our algorithms with `hmc` in the Pyhmc package, which is a standard HMC method, and `pystan` in the Pystan package, which is a no-U-turn implementation of HMC. For `hmc`, we set the sample size to be 1000, and for `pystan`, we set the number of iteration to be 2000. All the other parameters are set to be the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 3]( comp_stan.png \"Figure 3\")\n",
    "<center>Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 4]( comp_hmc.png \"Figure 4\")\n",
    "<center>Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3 and Figure 4 are the density plot for the `pystan` and `pyhmc` with the same sample size or number of iteration as ourselves algorithm. As we can see, the density plot of `pyhmc` is significantly different from the original plot. The two peaks of density are not symmetric with the $\\theta=0$. Moreover, for the plot of `pystan`, it looks like the original plots, but it is less condense at two peaks and more condense at the nadir when $\\theta=0$. Admittedly, we do not change the default setting for both algorithm and the performance may be better if we choose a more reasonable parameter. Also, the running time for `sghmc` is longer than both of the above two. But it did show that `sghmc` did pretty well for the s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chen, T., Fox, E., & Guestrin, C. (2014, January). Stochastic Gradient Hamiltonian Monte Carlo. In International Conference on Machine Learning (pp. 1683-1691).\n",
    "* Duane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987). Hybrid Monte Carlo. Physics letters B, 195(2), 216-222."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
