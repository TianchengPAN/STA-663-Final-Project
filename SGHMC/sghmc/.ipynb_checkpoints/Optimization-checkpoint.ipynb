{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./eigen'):\n",
    "    ! git clone https://gitlab.com/libeigen/eigen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "import time\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data, batch_size):\n",
    "    '''Define a function to compute the minibatches'''\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    \"Stochastic Gradient Hamiltonian Monte Carlo\"\n",
    "    by Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    ICML (2014).\n",
    "    \n",
    "    The inputs include:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    p = len(theta_0) \n",
    "    n = dat.shape[0] \n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size))) \n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta is not positive definite\")\n",
    "        return\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "    \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0 \n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch(dat, batch_size)\n",
    "        nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "        for batch in range(nbatches):\n",
    "            grad_U_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ grad_U_batch - alpha @ nu + \\\n",
    "                 np.random.multivariate_normal(np.zeros(p), Sigma).reshape(p, -1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplified algorithm\n",
    "def sghmc_simplified(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "    The inputs are:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    p = len(theta_0)\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta is not positive definite\")\n",
    "        return\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch(dat, batch_size)\n",
    "        nu = np.sqrt(eta_chol) @ np.random.normal(size=p).reshape(p,-1)\n",
    "        for batch in range(nbatches):\n",
    "            grad_U_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ grad_U_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization, try to use JIT first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def minibatch_numba(data, batch_size):\n",
    "    '''Define a function to compute the minibatches'''\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    \"Stochastic Gradient Hamiltonian Monte Carlo\"\n",
    "    by Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    ICML (2014).\n",
    "    \n",
    "    The inputs include:\n",
    "    grad_U = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    The return is a np.array of positions of theta.'''\n",
    "\n",
    "    p = len(theta_0)\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if np.all(np.linalg.eigvals(Sigma)) <= 0: \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = minibatch_numba(dat, batch_size)\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = grad_U(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparsion for mvn\n",
    "\n",
    "from autograd import jacobian\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "np.random.seed(1234)\n",
    "p = 2\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 200\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "theta_0 = theta\n",
    "eta = 0.01/n * np.eye(p)\n",
    "alpha = 0.1 * np.eye(p)\n",
    "V = np.eye(p)*1\n",
    "niter = 500\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting project.cpp\n"
     ]
    }
   ],
   "source": [
    "%%file project.cpp\n",
    "<%\n",
    "cfg['compiler_args'] = ['-std=c++11']\n",
    "cfg['include_dirs'] = ['eigen']\n",
    "setup_pybind11(cfg)\n",
    "%>\n",
    "\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/eigen.h>\n",
    "#include <stdexcept>\n",
    "#include <algorithm> // std::random_shuffle\n",
    "#include <random>\n",
    "\n",
    "#include <Eigen/LU>\n",
    "#include <Eigen/Dense>\n",
    "#include <eigen/Eigen/Core>\n",
    "\n",
    "namespace py = pybind11;\n",
    "using std::default_random_engine;\n",
    "using std::normal_distribution;\n",
    "        \n",
    "default_random_engine re{100};\n",
    "normal_distribution<double> norm(0, 1);\n",
    "auto rnorm = bind(norm, re);\n",
    "\n",
    "Eigen::MatrixXd rnorm_vec(int n) {\n",
    "    Eigen::MatrixXd res_vec = Eigen::MatrixXd::Zero(n, 1);\n",
    "    for (int i=0; i<n; i++) {\n",
    "        res_vec(i,0) = rnorm();\n",
    "    }\n",
    "    return res_vec;\n",
    "}\n",
    "    \n",
    "Eigen::MatrixXd gradU_noisyFig1(Eigen::MatrixXd theta) {\n",
    "    Eigen::MatrixXd xs = -4*theta.array() + 4*theta.array().pow(3) + 2*rnorm_vec(theta.rows()).array();\n",
    "    return xs;\n",
    "}\n",
    "     \n",
    "Eigen::MatrixXd gradU_mixNormals(Eigen::MatrixXd theta, Eigen::MatrixXd x, int n, int batch_size) {\n",
    "    int p = theta.rows();\n",
    "    Eigen::ArrayXd c_0 = theta(0,0) - x.array();\n",
    "    Eigen::ArrayXd c_1 = theta(1,0) - x.array();\n",
    "    Eigen::ArrayXd star = 0.5 * (-0.5 * c_0.pow(2)).exp() + 0.5 * (-0.5 * c_1.pow(2)).exp();\n",
    "    Eigen::ArrayXd star_prime;\n",
    "    Eigen::MatrixXd grad = Eigen::MatrixXd::Zero(p, 1);\n",
    "    for (int i=0; i<p; i++) {\n",
    "        star_prime = 0.5 * (-0.5 * (theta(i,0) - x.array()).pow(2)).exp() * (theta(i,0) - x.array());\n",
    "        grad(i,0) = -theta(i,0)/10 - (n/batch_size)*(star_prime/star).sum();\n",
    "    }\n",
    "    return grad;\n",
    "} \n",
    "\n",
    "Eigen::MatrixXd sghmc(std::string gradU_choice, Eigen::MatrixXd eta, int niter, Eigen::MatrixXd alpha, Eigen::MatrixXd theta_0, Eigen::MatrixXd V_hat, Eigen::MatrixXd dat, int batch_size){\n",
    "    int p = theta_0.rows();\n",
    "    int n = dat.rows();     \n",
    "    int p_dat = dat.cols();\n",
    "    int nbatches = n / batch_size; \n",
    "    Eigen::MatrixXd dat_temp = dat;\n",
    "    Eigen::MatrixXd dat_batch = Eigen::MatrixXd::Zero(batch_size, p_dat);\n",
    "    Eigen::MatrixXd gradU_batch = Eigen::MatrixXd::Zero(p, 1);\n",
    "    Eigen::MatrixXd theta_samps = Eigen::MatrixXd::Zero(p, niter*(n/batch_size));\n",
    "    std::vector<int> ind;\n",
    "    Eigen::MatrixXd beta_hat = 0.5 * V_hat * eta;\n",
    "    Eigen::MatrixXd Sigma = 2.0 * (alpha - beta_hat) * eta;\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfSig(Sigma); \n",
    "    if(lltOfSig.info() == Eigen::NumericalIssue){ \n",
    "        return theta_samps; \n",
    "    }\n",
    "    Eigen::MatrixXd Sig_chol = lltOfSig.matrixL(); \n",
    "    if(batch_size > n){ \n",
    "        return theta_samps; \n",
    "    }\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfeta(eta); \n",
    "    Eigen::MatrixXd eta_chol = lltOfeta.matrixL(); \n",
    "    Eigen::MatrixXd nu = eta_chol * rnorm_vec(p); \n",
    "    Eigen::MatrixXd theta = theta_0; \n",
    "    \n",
    "    int big_iter = 0;\n",
    "    for (int it=0; it<niter; it++) {\n",
    "        \n",
    "        Eigen::VectorXi indices = Eigen::VectorXi::LinSpaced(dat.rows(), 0, dat.rows());\n",
    "        std::random_shuffle(indices.data(), indices.data() + dat.rows());\n",
    "        dat_temp = indices.asPermutation() * dat;\n",
    "        nu = eta_chol * rnorm_vec(p);\n",
    "        int count_lower = 0;\n",
    "        int count_upper = batch_size;\n",
    "        for (int b=0; b<nbatches; b++){\n",
    "            int batch_ind = 0;\n",
    "            for (int ind_temp=count_lower; ind_temp<count_upper; ind_temp++){\n",
    "                dat_batch.row(batch_ind) = dat_temp.row(ind_temp);\n",
    "                batch_ind += 1;\n",
    "            }\n",
    "            count_lower += batch_size;\n",
    "            count_upper += batch_size;\n",
    "            if (gradU_choice == \"fig1\"){\n",
    "                gradU_batch = gradU_noisyFig1(theta);\n",
    "            } else if (gradU_choice == \"mixture_of_normals\"){\n",
    "                gradU_batch = gradU_mixNormals(theta, dat_batch, n, batch_size);\n",
    "            } else {\n",
    "                return theta_samps;\n",
    "            }\n",
    "            nu = nu - eta * gradU_batch - alpha * nu + Sig_chol * rnorm_vec(p);\n",
    "            theta = theta + nu;\n",
    "            theta_samps.col(big_iter) = theta;\n",
    "            big_iter += 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return theta_samps;\n",
    "}\n",
    "    \n",
    "PYBIND11_MODULE(project, m) {\n",
    "    m.doc() = \"auto-compiled c++ extension\";\n",
    "    m.def(\"sghmc\", &sghmc);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2 s ± 90.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_simplified(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-02c145d81987>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"sghmc_numba\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-144-02c145d81987> (17)\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 17:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "\n",
      "    p = len(theta_0)\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "<ipython-input-144-02c145d81987>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba\" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 17:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "\n",
      "    p = len(theta_0)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 17:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "\n",
      "    p = len(theta_0)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-144-02c145d81987>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-144-02c145d81987> (33)\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7 s ± 180 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test1.py\n"
     ]
    }
   ],
   "source": [
    "%%file test1.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "test = cppimport.imp(\"project\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    p = 2 \n",
    "    theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "    n = 200 \n",
    "    x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "                  np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "    theta_0 = theta\n",
    "    eta = 0.01/n * np.eye(p)\n",
    "    alpha = 0.1 * np.eye(p)\n",
    "    V = np.eye(p)*1\n",
    "    niter = 500\n",
    "    batch_size=50\n",
    "    times_all = np.zeros(7)\n",
    "    for i in range(7):\n",
    "        t0 = time.time()\n",
    "        samps_sghmc = test.sghmc(\"mixture_of_normals\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "        t1 = time.time()\n",
    "        times_all[i] = t1 - t0\n",
    "    print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "          \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019376618521554128 s ± 0.0003748492705152947 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python test1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparsion for fig 1\n",
    "\n",
    "import time\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "\n",
    "def U(theta):\n",
    "    return(-2*theta**2 + theta**4)\n",
    "gradU = jacobian(U, argnum=0)\n",
    "def noisy_gradU(theta, x, n, batch_size):\n",
    "    return -4*theta + 4*theta**3 + np.random.normal(0,2)\n",
    "\n",
    "np.random.seed(1234)\n",
    "n = 100\n",
    "x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "theta_0 = np.array([0.0])\n",
    "p = theta_0.shape[0]\n",
    "eta = 0.001 * np.eye(p) \n",
    "alpha = 0.01 * np.eye(p)\n",
    "V = np.eye(p)\n",
    "batch_size = n\n",
    "niter = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.179001126970565 s ± 0.04150309486923006 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "times_all = np.zeros(7)\n",
    "for i in range(7):\n",
    "    #original \n",
    "    t0 = time.time()\n",
    "    samps_sghmc = sghmc(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    times_all[i] = t1 - t0\n",
    "print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "      \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2300535270145962 s ± 0.01802794287233938 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "times_all = np.zeros(7)\n",
    "for i in range(7):\n",
    "    #simplified\n",
    "    t0 = time.time()\n",
    "    samps_sghmc = sghmc_simplified(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    times_all[i] = t1 - t0\n",
    "print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "      \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-02c145d81987>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"sghmc_numba\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-144-02c145d81987> (17)\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 17:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "\n",
      "    p = len(theta_0)\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "<ipython-input-144-02c145d81987>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba\" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 17:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "\n",
      "    p = len(theta_0)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 17:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "\n",
      "    p = len(theta_0)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-144-02c145d81987>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-144-02c145d81987> (33)\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-144-02c145d81987>\", line 33:\n",
      "def sghmc_numba(grad_U, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
      "    <source elided>\n",
      "    it = 0\n",
      "    for i in range(niter):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.451998233795166 s ± 0.8858564442586407 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "times_all = np.zeros(7)\n",
    "for i in range(7):\n",
    "    #numba\n",
    "    t0 = time.time()\n",
    "    samps_sghmc = sghmc_numba(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    times_all[i] = t1 - t0\n",
    "print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "      \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test2.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "test = cppimport.imp(\"project\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    n = 100\n",
    "    x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "    theta_0 = np.array([0.0]) \n",
    "    p = theta_0.shape[0]\n",
    "    eta = 0.001 * np.eye(p) \n",
    "    alpha = 0.01 * np.eye(p)\n",
    "    V = np.eye(p)\n",
    "    batch_size = n\n",
    "    niter = 50000 \n",
    "    \n",
    "    times_all = np.zeros(7)\n",
    "    for i in range(7):\n",
    "        t0 = time.time()\n",
    "        samps_sghmc = test.sghmc(\"fig1\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "        t1 = time.time()\n",
    "        times_all[i] = t1 - t0\n",
    "    print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "          \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20663997105189733 s ± 0.0038836895140119477 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
